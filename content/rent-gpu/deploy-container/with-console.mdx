import { Callout } from "nextra/components";
import { Steps } from 'nextra/components'

# Deploy Your Container on Spheron Console

This guide will walk you through the steps to deploy and access your application on Spheron Console. Follow these steps carefully to ensure a successful deployment.

<Steps>
### Access the Spheron Console
1. Visit [console.spheron.network](https://console.spheron.network)
2. Log in to your account or create a new one if you haven't already
3. Deposit some credits to your balance to pay for the deployment by clicking on the **Deposit** button in the top right corner.

### Select Your GPU
1. Navigate to the **Marketplace** tab
2. Choose between two deployment options:
    - **Secure GPUs**: Enterprise-grade hardware in professional data centers
      - Higher reliability and stability
      - Higher cost but guaranteed performance
    - **Community GPUs**: Shared resources from community members
      - More cost-effective
      - May have variable performance
3. Browse available GPUs or use the search function to find specific models
4. Review pricing and specifications before selection

### Deploy Template with SSH Access
This section will guide you through the process of deploying a template with SSH access.
<Steps>
### Generate SSH Key Pair
Generate an SSH key pair on your local machine, Follow the steps in [Generate SSH Keys](/rent-gpu/deploy-container/ssh-access)

### Upload Your Public Key to Spheron Console
Go to **User Settings > SSH Settings** in your Spheron console.

Upload the contents of your `id_rsa.pub` file.

### Deploy SSH Enabled Template
Select the **Pytorch** template with SSH enabled and configure your deployment settings:

- Adjust GPU count based on your needs
- Choose your deployment duration

### Enable SSH for Your Deployment
Once the deployment service is active, go to SSH tab and connect to your rented GPU. It will take few minutes to activate.

### Take the command from SSH tab and run it on your local machine

```sh
ssh -i <path-to-private-key> -p <port> root@<deployment-url>
```
</Steps>

### Deploy Template with No SSH Access  
This section will guide you through the process of deploying a template with no SSH access.
<Steps>
### Configure Your Deployment
1. Select the **Jupyter with PyTorch 2.4.1** template
   - Pre-configured with CUDA support
   - Includes all necessary dependencies for AI and LLM app development
2. Configure your deployment settings:
   - Set a secure password in the `JUPYTER_TOKEN` field
   - Adjust GPU count based on your needs
   - Choose your deployment duration
3. Review your configuration and click **Confirm**
4. Wait for deployment (typically under 60 seconds)

### Access Your Environment
1. Go to the **Overview** tab once deployment is complete
2. Locate the **py-cuda** service
3. Click the provided connection URL
4. Log in using your previously set password

<Callout type="warning">
  Make sure to save your JUPYTER_TOKEN password securely. You'll need it to access your environment.
</Callout>

### Start Developing
- Your environment comes pre-configured with:
  - Jupyter Notebook interface
  - PyTorch 2.4.1
  - CUDA support for GPU acceleration
  - Common ML/AI libraries
- All changes persist during your rental period
- You can install additional packages as needed
- You can also access the deployment shell to run any command you want and deployment logs to check the status of your deployment
</Steps>
</Steps>


## Verification
To verify GPU support:
1. Create a new Python notebook
2. Run the following code:
```python
import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
```
3. Or run this command to check the GPU count:
```bash
nvidia-smi
```

## Additional Tips
- Save your work regularly on Github.
- Monitor your memory usage carefully - if your notebook uses more memory than available (Out Of Memory/OOM), the server will automatically terminate and restart your notebook session, causing you to lose any unsaved work. You can check memory usage by running `nvidia-smi` in a notebook cell.
- Your deployment environment is dedicated to you and not shared with other users, ensuring optimal performance for your workloads.

Congratulations! You have successfully deployed your rented GPU and can now access it via SSH. If you encounter any issues, reach out to [Spheron Discord Support](https://sphn.wiki/discord).